---
title: "Problem Set 4"
author: "Sarah Morrison"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. We use (`*`) to indicate a problem that we think might be time consuming.

## Style Points (10 pts)

Please refer to the minilesson on code style [**here**](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-).

## Submission Steps (10 pts)

1.  This problem set is a paired problem set.
2.  Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    -   Partner 1 (Sarah Morrison morrisons):
    -   Partner 2 (Sarah Morrison morrisons):
3.  Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted.
4.  "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*SM\*\*
5.  "I have uploaded the names of anyone else other than my partner and I worked with on the problem set [**here**](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)" (1 point)
6.  Late coins used this pset: \*\*0\*\* Late coins left after submission: \*\*3\*\*
7.  Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`,
    -   The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate.
8.  (Partner 1): push `ps4.qmd` and `ps4.pdf` to your github repo.
9.  (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers.

## Download and explore the Provider of Services (POS) file (10 pts)

```{python}
import geopandas as gpd
import pandas as pd
import altair as alt
```

1.  PRVDR_CTGRY_CD, PRVDR_CTGRY_SBTYP_CD, FAC_NAME, PRVDR_NUM, ZIP_CD, CBSA-URBN-RRL-IND, CITY_NAME, STATE_CD, PGM_TRMNTN_CD

2.  
```{python}
# read in 2016 data
filepath = "/Users/sarahmorrison/Downloads/Github/problem-set-4-sarah/pos2016.csv"
pos2016 = gpd.read_file(filepath)
```

```{python}
# filter 2016 data for short-term hospitals
pos2016_sth = pos2016[(pos2016['PRVDR_CTGRY_SBTYP_CD']=='01') & (pos2016['PRVDR_CTGRY_CD']=='01')]
pos2016_sth.shape
# add year columb
pos2016_sth['year'] = 2016
```

    a.  There are 7245 hospitals reported in this data. The article says there are nearly 5000 short-term hospitals, so this number seems big.

    b.  The number could be bigger than the article says because of duplicate hospitals. Some hospitals are listed in the data twice because of a name change.

3.  
```{python}
# read in 2017 data
filepath = "/Users/sarahmorrison/Downloads/Github/problem-set-4-sarah/pos2017.csv"
pos2017 = gpd.read_file(filepath)
```

```{python}
# read in 2018 data
filepath2 = "/Users/sarahmorrison/Downloads/Github/problem-set-4-sarah/pos2018.csv"
pos2018 = gpd.read_file(filepath2, encoding='latin1') # helped by Ralph Valery Valiere ed discussion comment
```

```{python}
# read in 2019 data
filepath = "/Users/sarahmorrison/Downloads/Github/problem-set-4-sarah/pos2019.csv"
pos2019 = gpd.read_file(filepath, encoding='latin1')
```
```{python}
# filter 2019 data for short-term hospitals
pos2017_sth = pos2017[(pos2017['PRVDR_CTGRY_SBTYP_CD']=='01') & (pos2017['PRVDR_CTGRY_CD']=='01')]
pos2017_sth.shape
# add year column
pos2017_sth['year'] = 2017
# filter 2018 data for short-term hospitals
pos2018_sth = pos2018[(pos2018['PRVDR_CTGRY_SBTYP_CD']=='01') & (pos2018['PRVDR_CTGRY_CD']=='01')]
pos2018_sth.shape
# add year column
pos2018_sth['year'] = 2018
# filter 2019 data for short-term hospitals
pos2019_sth = pos2019[(pos2019['PRVDR_CTGRY_SBTYP_CD']=='01') & (pos2019['PRVDR_CTGRY_CD']=='01')]
pos2019_sth.shape
# add year column
pos2019_sth['year'] = 2019
```
```{python}
# append all four years
combined_pos = pd.concat([pos2016_sth, pos2017_sth, pos2018_sth, pos2019_sth], ignore_index=True)
```

```{python}
# create dataframe with number of observations per year
pos_yearly_observations = combined_pos.groupby('year').size().reset_index()
# rename number of observations column
pos_yearly_observations = pos_yearly_observations.rename(columns={0: 'hospitals'})
# create plot
pos_yearly_observations_chart = alt.Chart(pos_yearly_observations, title='Number of Short Term Hospitals per Year from 2016-2019').mark_bar().encode(
    alt.X('year:O', axis=alt.Axis(
        title='Year', labelAngle=0)),
    alt.Y('hospitals:Q', axis=alt.Axis(
        title='Number of Short Term Hospitals'
    ))
    ).properties(
        width=200
    )
# https://altair-viz.github.io/gallery/scatter_with_labels.html#gallery-scatter-with-labels
text = pos_yearly_observations_chart.mark_text(baseline='bottom').encode(text='hospitals')

pos_yearly_observations_chart + text
```

4.  
    a. 
```{python}
# ChatGPT Question: how to find the number of unique values in a column

# Find unique number of short term hospitals per year
unique_pos_2016 = pos2016_sth['PRVDR_NUM'].nunique()
unique_pos_2017 = pos2017_sth['PRVDR_NUM'].nunique()
unique_pos_2018 = pos2018_sth['PRVDR_NUM'].nunique()
unique_pos_2019 = pos2019_sth['PRVDR_NUM'].nunique()

# Create dataframe with years and the number of unique hospitals per year)
unique_pos_yearly = pd.DataFrame({
    'year': [2016, 2017, 2018, 2019],
    'unique_pos_count': [unique_pos_2016, unique_pos_2017, unique_pos_2018, unique_pos_2019]
})

# Create unique hospitals plot
unique_pos_yearly_chart = alt.Chart(unique_pos_yearly, title='Number of Unique Short Term Hospitals per Year from 2016-2019').mark_bar(size=30).encode(
    alt.X('year:O', axis=alt.Axis(
        title='Year', labelAngle=0)),
    alt.Y('unique_pos_count:Q', axis=alt.Axis(
        title='Number of Unique Short Term Hospitals'
    ))
    ).properties(
        width=300
    )
# https://altair-viz.github.io/gallery/scatter_with_labels.html#gallery-scatter-with-labels
text = unique_pos_yearly_chart.mark_text(baseline='bottom').encode(text='unique_pos_count:Q')

unique_pos_yearly_chart + text
```

    b.  There are the same number of hospitals even when you edit for unique CMS certification numbers. This tells us that the data entered is accurate and each hospital only has one record per year.

## Identify hospital closures in POS file (15 pts) (\*)

1.  
```{python}
# filter 2016 data for only active hospitals
active_2016 = pos2016_sth[pos2016_sth['PGM_TRMNTN_CD']=='00']

# filter 2016 data for only the information needed for the question
active_2016_list = active_2016[['PGM_TRMNTN_CD', 'PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']]

# ChatGPT Question: how to filter dataset based on value in another dataset
# create dataframes with all the active 2016 hospitals
# 2017
active_2017 = pos2017_sth[pos2017_sth['PRVDR_NUM'].isin(active_2016_list['PRVDR_NUM'])]
active_2017_list = active_2017[['PGM_TRMNTN_CD', 'PRVDR_NUM', 'FAC_NAME', 'ZIP_CD', 'year']]

# 2018
active_2018 = pos2018_sth[pos2018_sth['PRVDR_NUM'].isin(active_2016_list['PRVDR_NUM'])]
active_2018_list = active_2018[['PGM_TRMNTN_CD', 'PRVDR_NUM', 'FAC_NAME', 'ZIP_CD', 'year']]

# 2019
active_2019 = pos2019_sth[pos2019_sth['PRVDR_NUM'].isin(active_2016_list['PRVDR_NUM'])]
active_2019_list = active_2019[['PGM_TRMNTN_CD', 'PRVDR_NUM', 'FAC_NAME', 'ZIP_CD', 'year']]

# create a list to iterate over
active_list = [(2017, active_2017_list), (2018, active_2018_list), (2019, active_2019_list)]

# ChatGPT Question: how to make a function to find values that are not 00 in three dataframes
# create function 
def find_inactives(active_2016_list, active_list, PGM_TRMNTN_CD='00'):
    inactive_facilities_list = []
    for year, df, in active_list:
        merge_active = active_2016_list[['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']].merge(
            df[['PRVDR_NUM', 'PGM_TRMNTN_CD']], on='PRVDR_NUM', how='left'
        )
        inactives = merge_active[merge_active['PGM_TRMNTN_CD'] != PGM_TRMNTN_CD]
        inactives['year'] = year
        inactive_facilities_list.append(inactives[['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD', 'year']])
    inactive_facilities = pd.concat(inactive_facilities_list).drop_duplicates(subset='PRVDR_NUM')
    return inactive_facilities

inactive_facilities = find_inactives(active_2016_list, active_list, PGM_TRMNTN_CD='00')
print(inactive_facilities)

# How many hospitals are there that fit this definition?
inactive_facilities.shape
```

2.  
```{python}
# sort hospitals by name
sorted_inactive_facilities = inactive_facilities.sort_values(by='FAC_NAME')[['FAC_NAME', 'year']]

# show first 10
sorted_inactive_facilities.head(10)
```

3.  
```{python}
# fix function so that it removes codes 00 and 01
def find_closures(active_2016_list, active_list, closed_codes=['02', '03', '04', '07', '05', '06']):
    inactive_facilities_list = []
    for year, df, in active_list:
        merge_active = active_2016_list[['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']].merge(
            df[['PRVDR_NUM', 'PGM_TRMNTN_CD']], on='PRVDR_NUM', how='left'
        )
        inactives = merge_active[merge_active['PGM_TRMNTN_CD'].isin(closed_codes)]
        inactives['year'] = year
        inactive_facilities_list.append(inactives[['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD', 'year']])
    inactive_facilities = pd.concat(inactive_facilities_list).drop_duplicates(subset='PRVDR_NUM')
    return inactive_facilities

closed_facilities = find_closures(active_2016_list, active_list, closed_codes=['02', '03', '04', '07', '05', '06'])
print(closed_facilities)

closed_facilities.shape
```

    a.  Since there were 174 hospitals in the first list and 41 in the second, we can assume that 133 hospitals were actually mergers/acquisitions

    b.  After the correction, there are only 41 hospitals left.

    c.  
```{python}
# sort hospitals by name
sorted_closed_facilities = closed_facilities.sort_values(by='FAC_NAME')

# show first 10
sorted_closed_facilities.head(10)
```

## Download Census zip code shapefile (10 pt)

1.  

    a.  In the shapfile folder, there is a dBase database file, a projection definition file, a shapefile, a shape index format file, and an extensible markup language file. The shapefile stores the geometric data for the spatial features. The shape index file has the index for the geometric data for easy access. The database file has the attribute data in tabular form. The projection defintion file has the coordinate system for the shape file.

    b.  The .dbf file is 6.4 MB, the .prj file is 165 B, the .shp file is 837.5 MB, the .shx file is 265 KB, and the .xml file is 16 KB. The .shp is the biggest file.

2.  

```{python}
# read in shapefile
zip_shapefile = gpd.read_file("gz_2010_us_860_00_500k.zip")
```

```{python}
# filter shapefile for Texas zipcodes
texas_shapefile = zip_shapefile[zip_shapefile['ZCTA5'].str.startswith(('733','75', '76', '77', '78', '79'))]

# rename zipcode column to match pos file
texas_shapefile = texas_shapefile.rename(columns={'ZCTA5': 'ZIP_CD'})

# filter 2016 hospital for Texas zipcodes
pos2016_tx = pos2016_sth[pos2016_sth['ZIP_CD'].str.startswith(('733','75', '76', '77', '78'))]

# find number of hospitals per zipcode
hospital_count = pos2016_tx.groupby('ZIP_CD').size().reset_index(name='hospital_count')

# merge files
hospital_count_map = texas_shapefile.merge(hospital_count, on='ZIP_CD', how='left')

# preview data
hospital_count_map

# fill NAs with 0
hospital_count_map['hospital_count'] = hospital_count_map['hospital_count'].fillna(0)

hospital_count_map.plot(column='hospital_count', legend=True)
```

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (\*)

1.  

2.  

3.  

4.  

    a.  

    b.  

5.  

    a.  

    b.  

    c.  

## Effects of closures on access in Texas (15 pts)

1.  

2.  

3.  

4.  

## Reflecting on the exercise (10 pts)